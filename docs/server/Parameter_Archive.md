---
layout: default
permalink: /docs/server/Parameter_Archive/
sidebar: yes
---

## General description
The parameter archive stores for each parameter uples of (t<sub>i</sub>, ev<sub>i</sub>, rv<sub>i</sub>, ps<sub>i</sub>) where:

* t<sub>i</sub> - is a timestamp when the sample has been taken
* ev<sub>i</sub> - is the enginnering value of the parameter at the given time.
* rv<sub>i</sub> - is the enginnering value of the parameter at the given time.
* ps<sub>i</sub> - is the parameter status of the parameter at the given time.

The parameter status includes things like out of limits (alarms), processing status, etc. XTCE provides a mechanism through which a parameter can change it's alarm ranges depending on the context. For this reason we store in the parameter status also the applicable alarm ranges at the given time. 

In the remaining of the chapter we will use pv<sub>i</sub> to denote the triplet (ev<sub>i</sub>, rv<sub>i</sub>, ps<sub>i</sub>)

### Why not store the values on change only?
We can notice that in a typical space data stream there are many parameters that do not change very often (like an device ON/OFF status). For these, the space required to store the timestamp can greatly exceed in size the space required for storing the value (if simple compression is used).

In fact since the timestamps are 8 bytes long, they execed in size the parameter values almost in all cases, even for parameters that do change.

To reduce the size of the archive, some parameter archives store the values just when they change with respect to the previous value. Often (like in the device on/off example) the exact timestamps of the samples received in between the changes are not very important. Of course, one has always to take care that gaps in the data are not mistaken for non-changing parameter values.  
Storing the values on change only will reduce the space required not only for the value but also (and more importantly) for the timestamp.

However, we know that more often than not parameters are not sampled individually but in packets or frames, and many (if not all) the parameters from one packet share the same timestamp.

Usually some of the parameters in these packets will be counters or other things that do change with each sample. It follows that at least for storing those ever changing parameter samples, one has to store the timestamps anyway.

This is why, in Yamcs we do not adopt the "store on change only" strategy but a different one: we store the timestamps in one record and make reference to that record from all the parameters sharing those same timestamps. Of course it wouldn't make any sense to reference one singe timestamp value, instead we store multiple values in a segment and reference the time segment from all value segments that are related to it.

## Archive structure 
We have estabilished that the Yamcs parameter archive stores rows of data of shape:
(t, pv<sub>0</sub>, pv<sub>1</sub>, pv<sub>2</sub>,...,  pv<sub>n</sub>)

Where pv<sub>0</sub>, pv<sub>1</sub>, pv<sub>2</sub>..pv<sub>n</sub> are parameter values (for different parameters) all sharing the same timestamp t. One advantage of seeing the data this way is that we do keep together parameters extracted from the same packet (and having the same timestamp). It is sometimes useful for operators to know a sepcific parameter from which packet has been extracted (e.g. which APID, packet ID in a CCSDS packet structure).

The parameter archive partitions the data at two levels:

1.  time partitioned in partitions of 2^31 milliseconds duration (=~ 25 days). Each partition is stored in its own ColumnDataFamily in RocksDB (which means separate files and the possibility to remove an entire partition at a time).

2. Inside each partition, data is segmented in segments of 2^22 miliseconds (=~ 70 minutes) duration. One data segment contains all the engineering values or raw values or parameter status for one parameter. A time segment contains all the corresponding timestamps. 

This means that each parameter requires each ~70 minutes three segments for storing the raw, engineering and status plus a segment containing the timestamps. The timestamp segment is shared with other parameters. In order to be able to efficiently compress and work with the data, one segment stores data of one type only.

Each (parameter_fqn, eng_type, raw_type) combination is given an unique 4 bytes parameter_id (fqn= fully qualified name). We do this in order to be able to accomodate changes in parameter definitions in subsequent versions of the mission database (Xtce db).

The parameter_id=0 is reserved for the timestamp.

A ParameterGroup - represents a list of parameter_id which share the same timestamp.
Each ParameterGroup is given a ParameterGroup_id


Column Families
 for storing metadata we have 2 CFs:  
   meta_p2pid:  contains the mapping between parameter fully qualified name and parameter_id and type
   meta_pgid2pg: contains the mapping between ParameterGroup_id and parameter_id
 
For storing parameter values and timestamps we have 1CF per partition: data_<partition_id>.
partition_id is basetimestamp (i.e. the start timetamp of the 2^31 long partitions) in hexadecimal (without 0x in front)

Inside the data partitions we store (key,value) records where:
  key: parameter_id, ParameterGroup_id, type, segment_start_time (the type=0,1 or 2 for the eng value, raw value or parameter status)
  value: ValueSegment or TimeSegment (if parameter_id =0)
      
We can notice from this organization, that inside one partition, the segments containing data for one parameter follows in the rockdsdb files in sequence of engvalue<sub>segment_1</sub>,rawvalue<sub>segment_1</sub>,parameterstatus<sub>segment_1</sub>, engvalue<sub>segment_2</sub>. rawvalue<sub>segment_2</sub>...
      

### Segment encoding
The segments are compressed in different ways depending on their types.

**SortedTimeSegment** - stores the timestamps as uitn32 deltas from the beginning of the segment. The data is first encoded into deltas of deltas, then it's zigzag encoded (such that it becomes positive) and then it's encoded with FastPFOR and VarInt. FastPFOR encodes blocks of 128 bytes so VarInt encoding is used for the remaining data.

Storing timestamps as deltas of deltas helps if the data is sampled at regular intervals (especially by a real-time sytem). In this case the encoded deltas of deltas become very close to 0 and that compresse very well.

Description of the VarInt and zigzag encoding can be found in [Protocol Buffer docs](https://developers.google.com/protocol-buffers/docs/encoding).

Description and implementation of the FastPFOR algorithm can be found [here](https://github.com/lemire/JavaFastPFOR).

**IntSegment** - stores int32 or uint32 encoded same way as the time segment.

**FloatSegment** - stores 32 bits floating point numbers encoded using the algorithm (very slightly modified to work on 32 bits) described in the [Facebook Gorilla paper](http://www.vldb.org/pvldb/vol8/p1816-teller.pdf)

**ParameterStatusSegment**, **StringSegmnet** and **BinarySegment** are all stored either raw, as an enumeration, or run-length encoded, depeending on which results in smaller compressed size.

**DoubleSegment** and **LongSegment** are only stored as raw for the moment - compression is still to be implemented. For DoubleSegment we can employ the same approach like for 32 bits (since the original approach is in fact designed for compressing 64 bits floating point numbers).

## Archive filling 
There are two fillers that can be used to populate parameter archive:

* Realtime filling - the RealtimeFillerTask will subscribe to a realtime processor and write to the archive the parameter samples.
* Backfilling - the ArchiveFillerTask will create from time to time replays from the stream archive and write the generated parameters to the archive.

Due to the fact that data is stored in segments, one segment being a value in the (key,value) RocksDB, it is not efficient to write one "row" (data corresponding to one timestamp) at a time. It is much more efficient to collect data and write entire or at least partial segments at a time. 
The realtime filler will write the partial segments to the archive at each configurable interval. When retrieving data from the parameter archive, the latest (near realtime) data will be missing from the archive. That's why Yamcs uses the processor parameter cache to retrieve the near-realtime values.

The backFiller is by default enabled and it can also be used to issue rebuild requests via REST. The realtimeFiller has to be enabled in the configuration and the flushInterval (how often to flush the data in the archive) has to be specified. The flushInterval has to be smaller than the duration configured in the parameter cache.
The backFiller is configured with a so called warmupTime (by default 60 seconds) which means that when it performs a replay, it starts the replay earlier by the specified warmupTime amount. The reason is tha if there are any algorithms that depend on some parameters in the past for computing the current value, this should give them the chance to warmup. The data generated during the warmup is not stored in the archive (because it is part of the previous segment).



Example configuration for the parameter archive.

Parameter
<pre>
<code class="config-file">
services: 
    ....
    - class: org.yamcs.parameterarchive.ParameterArchive
      args: 
          realtimeFiller:
              enabled: true
              flushInterval: 300 #seconds
          backFiller:
	      #warmupTime: 60 seconds default warmupTime
              enabled: true
              schedule: [{startSegment: 10, numSegments: 3}]
</code>
</pre>
This configuration enables the realtime filler flushing the data to the archive each 5 minutes, and in addition the backFiller fills the archive 10 segments (approx 700 minutes) in the past, 3 segments at a time.

<pre>
<code class="config-file">
services: 
    ....
    - class: org.yamcs.parameterarchive.ParameterArchive
      args: 
          realtimeFiller:
              enabled: false              
          backFiller:
              enabled: true
              warmupTime: 120 
              schedule: [{startSegment: 10, numSegments: 3}, {startSegment: 2, numSegments:2, interval: 600} ]
</code>
</pre>
This configuration does not use the realtime filler, but instead performs regular (each 600 seconds) back-fillings of the last two segments. It is the configuration used in the ISS ground segment where due to regular(each 20-30min) LOS (loss of signal), the archive is very fragmented and the only way to obtain continous data is to perform replays.

### Future work
It would be desirable to backfill only parts of the archive. Indeed, some ground generated data may not suffer necessarily of gaps and could be just realtime filled. Currently there is no possibility to specify what parts of the archive to be back-filled. This may be implemented in a future version.
Another useful feature would be to trigger the back filling automatically when gaps are filled in the stream archive. This will hopefully also be implemented in a future version  (contributions welcome!).
